## [Project] MNIST 숫자 분류: LeNet-5 구현과 성능 최적화 기록

---

### 1. 프로젝트 개요
MNIST 데이터셋으로 손글씨 숫자를 분류하는 딥러닝 모델을 만들어보았습니다. 단순히 예제 코드를 따라 치는 것에서 벗어나, **LeNet-5 아키텍처를 직접 구현해보고 다양한 최적화 기법을 적용해 성능을 어디까지 끌어올릴 수 있는지** 실험해보는 데 초점을 맞췄습니다.

* **목표:** LeNet-5 구현 및 데이터 증강/튜닝을 통한 정확도 극대화
* **사용 기술:** PyTorch, Optuna, scikit-learn
* **최종 결과:** 테스트 정확도 **99.28%**

---

### 2. 주요 시도

**① 99%의 벽을 넘기 위한 튜닝 (Optuna)**
처음 LeNet-5를 돌렸을 때 정확도는 약 99.00%였습니다. 여기서 더 올리는 게 쉽지 않았는데, Optuna를 도입해 학습률(Learning rate), 배치 사이즈, 드롭아웃 비율을 자동으로 탐색하게 했습니다. 특히 학습 중간에 검증 손실(Validation Loss)이 줄어들지 않으면 학습률을 낮춰주는 스케줄러를 적용했더니, 모델이 훨씬 안정적으로 수렴하며 **99.28%**까지 성능이 올랐습니다. 이론으로만 알던 'Learning Rate Decay'의 효과를 직접 체감함

**② 단순한 데이터일수록 중요했던 '증강(Augmentation)'**
MNIST는 워낙 깔끔한 데이터라 굳이 이미지를 회전하거나 늘릴 필요가 있을까 싶었으나 랜덤 회전과 크기 조정을 적용해보니, 미세하지만 일반화 성능이 좋아지는 걸 확인했습니다. 쉬운 데이터셋이라도 다양성을 확보하는 게 모델을 더 '단단하게(Robust)' 만든다는 것을 확인할 수 있었습니다. 

**③ 운에 맡기지 않는 검증 (Cross Validation)**
데이터를 한 번만 쪼개서 검증하다 보면, 운 좋게 잘 맞는 데이터만 걸릴 수도 있다는 생각이 들었습니다. 그래서 K-Fold 교차 검증을 도입했고, 덕분에 특정 데이터셋에 과적합되지 않은 객관적인 성능 지표를 얻을 수 있었습니다.

**④ 나중을 위한 코드 정리**
혼자 하는 프로젝트지만, 나중에 코드를 다시 보거나 다른 프로젝트에 가져다 쓸 때를 대비해 함수와 클래스 구조를 신경 써서 짰습니다. 매개변수를 하드코딩하지 않고 명시적으로 뺀 덕분에 실험할 때 수정이 훨씬 편했습니다.

---

### 3. 성능 분석
* **최종 정확도:** 99.28%
* **최적 파라미터:** LR 0.0029, Batch 256, Dropout 0.235, Epoch 18
* **오답 분석:** 대부분 잘 맞추지만, 사람이 봐도 헷갈리는 악필(3과 5, 4와 9 등)에서 오분류가 발생했습니다. LeNet-5 같은 얕은 구조로는 형태학적 특징을 완벽히 잡아내기엔 한계가 있어 보입니다. 추후엔 ResNet 같은 더 깊은 모델로 이 0.7%의 오차를 줄여보고 싶습니다.

---

### 4. 기술 스택 및 디렉토리 구조
* **Framework:** PyTorch
* **Libs:** torchvision, optuna, scikit-learn, matplotlib
* **Structure:**
    ```bash
    mnist-classification/
    │
    ├── data/               # 데이터셋 저장
    ├── outputs/            # 모델 가중치 및 학습 로그 그래프
    ├── lenet5.py           # 모델 구현 및 학습 메인 코드
    ├── requirements.txt    # 의존성 패키지 목록
    └── README.md           # 현재 파일
    ```

---

### 참고 자료
프로젝트를 진행하는 데 참고했던 자료들
* [MNIST 데이터셋](http://yann.lecun.com/exdb/mnist/)
* [PyTorch 공식 문서](https://pytorch.org/docs/)
* [합성곱 신경망(CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network)
* [Optuna 공식 문서](https://optuna.readthedocs.io/en/stable/)
