{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from tqdm.notebook import tqdm  # tqdm.auto 대신 tqdm.notebook 사용\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. 시드 고정 (재현성 확보)\n",
    "# ========================\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 변환(Transforms)에서의 난수\n",
    "- 데이터 증강(transforms.RandomRotation, transforms.RandomResizedCrop, transforms.RandomAffine) 과정에서 난수를 사용하여 이미지를 무작위로 회전, 크기 조정, 이동시킵니다.\n",
    "\n",
    "- 가우시안 노이즈(AddGaussianNoise)도 난수를 통해 추가됩니다.\n",
    "\n",
    "→ 시드를 고정하지 않으면 매 학습 시도마다 데이터 증강 결과가 달라지고, 모델 학습 결과가 달라질 수 있습니다.\n",
    "\n",
    "### 2. K-Fold 작업에서의 난수\n",
    "- K-Fold 교차 검증에서 데이터를 무작위로 섞기(shuffle=True) 위해 난수가 필요합니다.\n",
    "\n",
    "- random_state를 설정해 시드를 고정하면 동일한 데이터 분할을 재현할 수 있습니다.\n",
    "\n",
    "→ 시드를 고정하지 않으면 각 fold에서 데이터셋 분할이 매번 달라지며, 실험 결과를 비교하기 어렵습니다.\n",
    "\n",
    "### 3. 딥러닝 모델의 내부 난수\n",
    "- 모델 가중치 초기화(초기값은 난수로 설정).\n",
    "\n",
    "- 드롭아웃(Dropout)에서의 노드 선택.\n",
    "\n",
    "- 데이터 로더에서 shuffle=True 옵션.\n",
    "\n",
    "→ 시드를 고정하지 않으면 모델 학습의 시작 조건이 매번 달라지고, 결과가 일관되지 않을 수 있습니다.\n",
    "\n",
    "### 요약\n",
    "- 시드를 고정하는 이유는 데이터를 증강하거나 분할할 때 발생하는 난수의 일관성을 유지하여, 실험 결과의 재현성을 확보하고 신뢰성을 높이기 위해서입니다.\n",
    "이 과정은 특히 딥러닝 연구나 하이퍼파라미터 튜닝(예: Optuna)에서 필수적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 2. 데이터 증강 클래스 정의 (가우시안 노이즈 추가)\n",
    "# ========================\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.1):\n",
    "        self.mean = mean # 평균 mean 0 \n",
    "        self.std = std # 표준 편차 standard deviation 0.1 \n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        # 기존 텐서에 가우시안 노이즈를 더해서 리턴\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # __repr__은 객체의 디버깅용 표현을 정의하며, repr() 함수 호출, 대화형 환경, 디버깅 상황에서 자동으로 호출됩니다. 이를 통해 객체의 내부 정보를 명확히 확인할 수 있습니다.\n",
    "        # f'(mean={self.mean}, std={self.std})' Python의 f-string(포맷 문자열)을 사용하여 문자열을 동적으로 생성하는 표현식\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __call__\n",
    "- __call__은 Python에서 객체를 함수처럼 호출할 수 있도록 해주는 특별 메서드\n",
    "### __repr__\n",
    "- 객체의 표현(representation)을 정의하는 메서드\n",
    "- 객체에 대한 명확하고 디버깅 친화적인 문자열을 반환.\n",
    "- 객체의 상태나 중요한 정보를 포함하여, 개발자가 객체를 이해하는 데 도움을 줌.\n",
    "- 주로 개발자 도구와 관련된 환경에서 사용.\n",
    "\n",
    "### f-string\n",
    "Python 3.6부터 지원되는 기능으로, 문자열 내에서 중괄호 {}를 사용해 변수나 표현식의 값을 삽입할 수 있습니다.\n",
    "\n",
    "```python\n",
    "f\"문자열 {변수 또는 표현식}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 3. 데이터 로드 및 전처리 (교차 검증 포함)\n",
    "# ========================\n",
    "def load_data(batch_size=64):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomResizedCrop(\n",
    "            size=28,\n",
    "            scale=(0.8, 1.2),\n",
    "            ratio=(0.9, 1.1)\n",
    "        ),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=0,\n",
    "            translate=(0.1, 0.1)\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        AddGaussianNoise(\n",
    "            mean=0.,\n",
    "            std=0.1\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5,),\n",
    "            std=(0.5,)\n",
    "        )\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5,),\n",
    "            std=(0.5,)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # 전체 학습 데이터셋 로드\n",
    "    full_train_dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        transform=transform_train,\n",
    "        download=True\n",
    "    )\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        transform=transform_test,\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    return full_train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading & preprocessing\n",
    "- load_data 함수는 **\"훈련 데이터 셋, 평가 데이터 셋\"을 튜플 형태**로 반환하는 함수.\n",
    "\n",
    "#### Data augmentation (데이터 증강) 처리 추가\n",
    "- Rotation, Affine Transformation을 통해 데이터 증강하여 *overfitting* 방지, *robustness* 증대, *Generalization* 성능을 기대할 수 있다.\n",
    "- AddGaussianNoise을 통해 Noise 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 4. 모델 정의 (LeNet5)\n",
    "# ========================\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(\n",
    "            num_features=6\n",
    "        )\n",
    "        self.pool1 = nn.AvgPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=16,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(\n",
    "            num_features=16\n",
    "        )\n",
    "        self.pool2 = nn.AvgPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=120,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=120,\n",
    "            out_features=84\n",
    "        )\n",
    "        self.dropout = nn.Dropout(\n",
    "            p=dropout_rate\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=84,\n",
    "            out_features=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (2, 2, 2, 2))  # 입력 이미지를 32x32로 패딩\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # 플래튼\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 Modeling\n",
    "**1 - 1st conv : 커널 사이즈 5x5**\n",
    "- 입력 : 1채널의 32x32 이미지\n",
    "- 출력 : 6채널의 28x28 이미지\n",
    "- *stride=1*은 컨볼루션 연산 시 건너 뛰는 간격으로, 보통 모든 픽셀에 대해 컨볼루션 수행하므로 1로 설정함\n",
    "\n",
    "**RELU 함수 적용1**\n",
    "\n",
    "**2 - 1st pool : 커널 사이즈 2x2**\n",
    "- 입력 : 6채널의 28x28 이미지\n",
    "- 출력 : 6채널의 14x14 이미지\n",
    "\n",
    "**3 - 2nd conv : 커널 사이즈 5x5**\n",
    "- 입력 : 6채널의 14x14 이미지 \n",
    "- 출력 : 16채널의 10x10 이미지 (padding 없이 5x5 커널을 사용했으므로 상하좌우로 2라인씩 줄어들어 10x10이 됨)\n",
    "-> 6x14x14에 *16개의 5x5 커널*과 컨볼루션 연산 하면 16x10x10이 됨\n",
    "\n",
    "**RELU 함수 적용2**\n",
    "\n",
    "**4 - 2nd pool : 커널 사이즈 2x2**\n",
    "- 입력 : 16채널의 10x10 이미지\n",
    "- 출력 : 16채널의 5x5 이미지\n",
    "\n",
    "**5 - 3rd conv : 커널 사이즈 5x5**\n",
    "- 입력 : 16채널의 5x5 이미지 \n",
    "- 출력 : 120채널의 1x1 이미지\n",
    "\n",
    "**RELU 함수 적용3**\n",
    "\n",
    "**conv 결과에 Flatten 처리**\n",
    "\n",
    "**6 - FCL1**\n",
    "- 입력 : 120 차원 벡터 [[x1, x2, x3, ... ,x120]]\n",
    "- 출력 : 84 차원 벡터 [[x1, x2, x3, ... ,x84]]\n",
    "\n",
    "**RELU 함수 적용4**\n",
    "\n",
    "**7 - FCL2**\n",
    "- 입력 : 84 차원 벡터 [[x1, x2, x3, ... ,x84]]\n",
    "- 출력 : 10 차원 벡터 [[x1, x2, x3, ... ,x10]] **-> logit**\n",
    "\n",
    "---\n",
    "#### 멀티 채널에서의 컨볼루션 연산 \n",
    "입력: 32×32x3\n",
    "필터: 3x5×5, 총 6개.\n",
    "출력: 28x28x6\n",
    "하나의 필터는 입력 데이터의 모든 채널(3채널)에 대해 합성곱 연산을 수행한 후, 결과를 합산하여 하나의 피처맵 생성.\n",
    "6개의 필터가 독립적으로 작동하여 총 6개의 피처맵 생성. \n",
    "\n",
    "---\n",
    "#### Flatten 처리\n",
    "- Fully Connected Layer를 통과하기 전에 Flatten 처리를 함\n",
    "- x.view(x.size(0), -1)\n",
    "- x는 120x1x1의 3차원 텐서이고, x.size(0)는 배치 사이즈를 의미 64\n",
    "- 64x120 크기의 2차원 텐서로 평탄화 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 5. 학습 함수 정의\n",
    "# ========================\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=5, early_stopping_patience=5):\n",
    "    # 함수 정의 시 설정한 default값 (num_epochs=5, early_stopping_patience=5)은 함수 호출 시 전달한 인자 값으로 대체됨\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device) # 모델 연산 device로 이동\n",
    "\n",
    "    # early stopping을 위한 loss value 및 몇 번 이상 loss 개선 안될 때 조기 종료할지 카운트하는 counter 초기화\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # 최적의 epoch 만큼 학습을 반복, 반복할 때 마다 모델은 초기화되지 않고 학습이 누적됨\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # 모델을 훈련 모드로 변경\n",
    "        total_loss = 0 # 누적 loss 값 0으로 초기화\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"): # 객체 train_loader로부터 images와 labels를 차례대로 꺼내는 걸 반복\n",
    "            images, labels = images.to(device), labels.to(device) # 꺼낸 images와 labels를 연산 device로 계속 보냄\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images) # 모델의 출력 값 \n",
    "            loss = criterion(outputs, labels) # 모델의 출력 값과 정답 labels를 비교해서 loss를 계산\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad() # 기울기값 초기화\n",
    "            loss.backward() # 기울기 계산\n",
    "            optimizer.step() # 계산한 기울기 이용하여 Fully connected layers 가중치 파라미터 업데이트\n",
    "\n",
    "            total_loss += loss.item() # 계산된 loss를 python에서 계산할 수 있게 변환해서 total_loss에 누적 시킴\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader) # 전체 훈련 데이터셋의 길이로 나누어 평균 loss를 계산\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval() # 모델을 평가 모드로 변경\n",
    "        total_val_loss = 0 # 누적 loss 값을 초기화\n",
    "        with torch.no_grad(): # 기울기 계산 하지 않음 \n",
    "            for images, labels in tqdm(val_loader, desc=\"Validation\"): # main에서 호출했을때 val_loader는 test_dataset을 로드한 test_loader임.cross_validate에서 호출했을때는 k-fold에서의 validate data임\n",
    "                images, labels = images.to(device), labels.to(device) # 검증 data에서 images와 labels를 연산 device로 보냄\n",
    "                outputs = model(images) # 평가용 모델의 출력 \n",
    "                loss = criterion(outputs, labels) # 정답과 비교하여 loss를 계산\n",
    "                total_val_loss += loss.item() # total_val_loss에 누적 loss를 저장\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader) # 평가용 평균 loss를 계산 \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\") # 지금 몇 epoch이고, 훈련 시엔 평균 몇 loss인지, 평가 시엔 평균 몇 loss인지를 출력\n",
    "        \n",
    "        # 스케줄러 업데이트 (ReduceLROnPlateau의 경우)\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau): # 학습률 스케줄러(scheduler)가 ReduceLROnPlateau 클래스인지 확인\n",
    "            scheduler.step(avg_val_loss) # ReduceLROnPlateau이면 평균 검증 loss 를 사용해서 스케줄러를 업데이트 해라 \n",
    "        else:\n",
    "            scheduler.step() # 아니라면 step() 메서드를 호출. 이 경우 학습률 스케줄러는 ReduceLROnPlateau와는 다른 방식으로 동작\n",
    "        \n",
    "        # 조기 종료 조건 확인\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 모델\n",
    "- 현재 구현 기준으로 train_model은 2번 사용된다. \n",
    "\n",
    "1. 하이퍼파라미터 최적화 시 교차 검증(K-FOLD)에서 호출되어 사용된다.\n",
    "2. 최적의 하이퍼파라미터를 찾은 후 해당 파라미터로 다시 모델을 학습 시킬 때 호출되어 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 6. 평가 함수 정의\n",
    "# ========================\n",
    "def evaluate_model_accuracy(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 모델\n",
    "- 모델의 정확도를 계산하기 위해 사용된다. \n",
    "\n",
    "현재 구현 기준으로는 2번 사용된다. \n",
    "1. 하이퍼파라미터 최적화 시 교차 검증(K-FOLD)에서 호출되어 사용된다.\n",
    "2. 최적의 하이퍼파라미터를 찾은 후 학습이 끝난 후 최종 평가 시 호출되어 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 7. 교차 검증 함수 정의\n",
    "# ========================\n",
    "def cross_validate(model_class, dataset, k=5, params=None):\n",
    "    # 교차검증 함수에 필요한 인자 : 모델 이름, 전체 training dataset, 5개의 fold, 최적화가 완료된 하이퍼파리미터들\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 연산 device 결정\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42) # 스킷런 라이브러리로 부터 가져온 KFOLD 메서드를 이용하여 교차 검증 객체를 생성\n",
    "    # shuffle=True 는 데이터를 K개의 Fold로 나누기 전에 데이터를 섞는다는 의미\n",
    "    # 이미 set_seed(seed=42) 함수에서 난수 시드를 고정했지만, random_state=42를 추가로 설정하는 이유는 scikit-learn과 PyTorch의 난수 생성기가 독립적으로 동작하기 때문. 서로 독립이므로 42가 아닌 다른 수를 설정해도 됨.\n",
    "    \n",
    "    accuracies = [] # fold마다 나오는 accuray들을 저장할 빈 리스트를 생성 k가 5면 5개의 accuracy가 저장됨 (accuracies = [0.85, 0.88, 0.86, 0.90, 0.87])\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"Fold {fold + 1}\") # fold는 0부터 시작하는 인덱스이므로 +1해서 몇 번째 fold 결과인지 출력\n",
    "        # 데이터 로더 생성\n",
    "\n",
    "        # train_idx와val_idx는 kfold로 분할된 데이터의 인덱스가 저장된 배열\n",
    "        # train_idx = ([1, 2, 3, 4, 5, ... ])\n",
    "        # val_idx = ([4000, 4001, 4002, ... ])\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=params['batch_size'],\n",
    "            sampler=train_subsampler\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=params['batch_size'],\n",
    "            sampler=val_subsampler\n",
    "        )\n",
    "        \n",
    "        # 교차 검증 마다 모델과 학습률 스케줄러가 초기화 되어야 각각의 독립적인 accuracy에 대해 구할 수 있으므로 \n",
    "        # 모델 초기화\n",
    "        model = model_class(dropout_rate=params['dropout_rate']).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # 학습률 스케줄러 초기화 (ReduceLROnPlateau)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            patience=params['scheduler_patience'],\n",
    "            factor=params['scheduler_factor'],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # 모델 학습\n",
    "        train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            num_epochs=params['num_epochs'],\n",
    "            early_stopping_patience=params['early_stopping_patience']\n",
    "        )\n",
    "        \n",
    "        # 모델 평가\n",
    "        accuracy = evaluate_model_accuracy(model, val_loader)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Fold {fold + 1} Accuracy: {accuracy:.2f}%\\n\")\n",
    "    \n",
    "    average_accuracy = sum(accuracies) / len(accuracies)\n",
    "    print(f\"Average Accuracy over {k} folds: {average_accuracy:.2f}%\")\n",
    "    # 교차 검증으로 나온 K개의 accuracy의 평균을 리턴 \n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 검증 (K-Fold Cross-Validation)\n",
    "- 교차 검증은 데이터셋을 여러 번 나누어 학습 및 검증 과정을 반복함으로써 모델의 일반화 성능을 평가하는 방법.\n",
    "- 여기서는 K-Fold 방법을 사용.\n",
    "\n",
    "\n",
    "### Code \n",
    "**for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):**\n",
    "- enumerate는 kfold.split(dataset)가 반복될 때 마다 그 횟수 정보를 추가하여 튜플 형태로 제공 (횟수, kfold.split(dataset)의 반환 값)\n",
    "- kfold.split(dataset)는 dataset를 훈련 셋과 검증 셋으로 나누었을 때 인덱스를 튜플로 제공 (훈련 셋 인덱스, 검증 셋 인덱스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 8. Optuna를 사용한 하이퍼파라미터 튜닝 (교차 검증 포함)\n",
    "# ========================\n",
    "def objective(trial):\n",
    "    try:\n",
    "        # 하이퍼파라미터 제안\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "        num_epochs = trial.suggest_int('num_epochs', 5, 20)\n",
    "        early_stopping_patience = trial.suggest_int('early_stopping_patience', 3, 10)\n",
    "        \n",
    "        # ReduceLROnPlateau의 하이퍼파라미터 제안 (학습률 스케줄러를 위한 하이퍼파라미터)\n",
    "        scheduler_patience = trial.suggest_int('scheduler_patience', 2, 5) # 성능 개선이 없더라도 학습률을 감소시키기 전에 기다리는 epoch 수\n",
    "        scheduler_factor = trial.suggest_float('scheduler_factor', 0.1, 0.5, step=0.1) # 학습률을 감소시킬 비율\n",
    "        \n",
    "        # 데이터 로드 - 하이퍼파라미터 최적화 시 훈련 셋만 필요\n",
    "        full_train_dataset, _ = load_data(batch_size)\n",
    "        \n",
    "        # 교차 검증 수행\n",
    "        params = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'num_epochs': num_epochs,\n",
    "            'early_stopping_patience': early_stopping_patience,\n",
    "            'scheduler_patience': scheduler_patience,\n",
    "            'scheduler_factor': scheduler_factor\n",
    "        }\n",
    "        \n",
    "        # 하이퍼 파라미터 최적화 시 k-fold를 이용 \n",
    "        average_accuracy = cross_validate(LeNet5, full_train_dataset, k=2, params=params) # 시간 단축을 위해 k를 5에서 2로 \n",
    "        # Kfold의 평균 정확도를 리턴 \n",
    "        return average_accuracy\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during trial: {e}\")\n",
    "        return 0  # 에러 발생 시 최소 정확도로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau는 모델의 학습 중 성능 개선이 정체되었을 때, 학습률을 감소시켜 학습이 더 잘 진행되도록 돕는 스케줄러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 9. Optuna 스터디 생성 및 최적화 실행\n",
    "# ========================\n",
    "def run_optuna_study(n_trials=20):\n",
    "    # Optuna 스터디 생성 (목표는 최대화, 즉 정확도 accuracy의 최대화)\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # 최적화 실행\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    if study.best_trial is not None:\n",
    "        print(\"Best trial:\")\n",
    "        trial = study.best_trial\n",
    "\n",
    "        print(f\"  Accuracy: {trial.value:.2f}%\")\n",
    "        print(\"  Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        \n",
    "        # 하이퍼파라미터 저장 \n",
    "        # indent=4는 JSON 파일을 사람이 읽기 쉽도록 들여쓰기(4칸)로 저장하는 옵션\n",
    "        # Context Manager 파일 읽기/쓰기 작업 - best_hyperparameters.json 파일 생성 및 저장\n",
    "        with open('outputs/best_hyperparameters.json', 'w') as f:\n",
    "            json.dump(trial.params, f, indent=4)\n",
    "        print(\"하이퍼파라미터가 'outputs/best_hyperparameters.json' 파일로 저장되었습니다.\")\n",
    "        \n",
    "        return trial.params, study\n",
    "    else:\n",
    "        print(\"No successful trials found.\")\n",
    "        return None, study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optuna study의 생성\n",
    "- Optuna에서 study를 create한다는 것은 하이퍼파라미터 최적화를 수행하기 위한 환경(스터디 객체)을 만드는 것\n",
    "\n",
    "### study의 목표\n",
    "- \"direction = maximize\"는 study의 목표가 accuracy 최대화임을 의미한다. \n",
    "- 정확도(accuracy)를 최적화하려면 maximize, 손실(loss)을 최적화하려면 minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 10. 모델 저장 및 불러오기\n",
    "# ========================\n",
    "def save_model(model, path='outputs/best_lenet5_mnist.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"모델이 '{path}' 파일로 저장되었습니다.\")\n",
    "\n",
    "def load_model(path='outputs/best_lenet5_mnist.pth', dropout_rate=0.5):\n",
    "    model = LeNet5(dropout_rate=dropout_rate)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"모델이 '{path}' 파일에서 불러와졌습니다.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 11. 예측 시각화\n",
    "# ========================\n",
    "def visualize_predictions(model, test_loader, num_images=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    images, labels = images[:num_images].to(device), labels[:num_images].to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(15, 6))\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        # 원본 이미지\n",
    "        ax = axes[0, idx]\n",
    "        img = images[idx].cpu().squeeze()\n",
    "        img = img * 0.5 + 0.5  # 정규화 해제\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f'Label: {labels[idx].item()}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # 예측된 이미지\n",
    "        ax = axes[1, idx]\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f'Pred: {preds[idx].item()}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 12. 메인 함수 정의\n",
    "# ========================\n",
    "def main():\n",
    "    # Optuna 하이퍼파라미터 튜닝 실행, n_trials번 동안 학습하면서 파라미터를 최적화 함\n",
    "    best_params, study = run_optuna_study(n_trials=2) # 동작 확인을 위한 시간 단축 20 -> 2로 수정\n",
    "    \n",
    "    if best_params is not None: # best hyperparameters를 찾았다면, \n",
    "        # 데이터 로드\n",
    "        full_train_dataset, test_dataset = load_data(best_params['batch_size']) # 전체 훈련 데이터, 평가 데이터를 batch size에 맞게 로드해라.\n",
    "        \n",
    "        # 전체 학습 데이터 로드\n",
    "        # Optuna에서 사용한 전체 학습 데이터를 다시 사용하지 않고, 전체 데이터로 다시 학습\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            full_train_dataset,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            shuffle=True\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # 모델 초기화\n",
    "        best_model = LeNet5(dropout_rate=best_params['dropout_rate'])\n",
    "        \n",
    "        # 손실 함수 및 최적화 알고리즘\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "        \n",
    "        # 학습률 스케줄러 초기화 (ReduceLROnPlateau)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            patience=best_params['scheduler_patience'],\n",
    "            factor=best_params['scheduler_factor'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # 모델 학습\n",
    "        train_model(\n",
    "            best_model,\n",
    "            train_loader,\n",
    "            test_loader,  # 여기서는 검증 데이터를 테스트 데이터로 사용\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            num_epochs=best_params['num_epochs'],\n",
    "            early_stopping_patience=best_params['early_stopping_patience']\n",
    "        )\n",
    "        \n",
    "        # 최종 평가\n",
    "        final_accuracy = evaluate_model_accuracy(best_model, test_loader)\n",
    "        print(f\"Final Accuracy with Best Params: {final_accuracy:.2f}%\")\n",
    "        \n",
    "        # 예측 시각화\n",
    "        visualize_predictions(best_model, test_loader, num_images=5)\n",
    "        \n",
    "        # 모델 저장\n",
    "        save_model(best_model, 'outputs/best_lenet5_mnist.pth')\n",
    "        \n",
    "        # Optuna 시각화\n",
    "        fig1 = plot_optimization_history(study)\n",
    "        fig1.write_image('outputs/optimization_history.png')\n",
    "        print(\"Optuna 최적화 히스토리가 'outputs/optimization_history.png'로 저장되었습니다.\")\n",
    "\n",
    "        fig2 = plot_param_importances(study)\n",
    "        fig2.write_image('outputs/param_importances.png')\n",
    "        print(\"Optuna 파라미터 중요도가 'outputs/param_importances.png'로 저장되었습니다.\")\n",
    "\n",
    "    else:\n",
    "        print(\"최적의 하이퍼파라미터를 찾지 못했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 하이퍼 파라미터 최적화 (Optuna를 이용, k-fold로 분류된 데이터 활용)\n",
    "2. training data / test data 로드 (최적화된 batch size)\n",
    "3. LeNet5 객체 생성 (최적화된 dropout)\n",
    "4. 손실 함수 및 최적화 알고리즘 객체 생성 (최적화된 하이퍼파라미터)\n",
    "5. 학습률 스케줄러 객체 생성 (최적화된 스케줄러의 하이퍼 파라미터)\n",
    "6. 모델 학습 (최적화된 하이퍼 파라미터)\n",
    "7. 모델 평가 및 성능 계산 \n",
    "8. 시각화 및 모델 저장\n",
    "9. 하이퍼파라미터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 디렉토리 생성\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
