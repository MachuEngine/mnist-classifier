{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # 기본 파이토치 기능\n",
    "import torch.nn as nn # nn 모듈 기능\n",
    "import torch.nn.functional as F # 기본 신경망 함수\n",
    "import torch.optim as optim # 최적화\n",
    "from torchvision import datasets, transforms # 데이터셋 처리\n",
    "import matplotlib.pyplot as plt # 데이터 시각화\n",
    "import optuna # 하이퍼파라미터 튜닝 라이브러리\n",
    "\n",
    "# ========================\n",
    "# 1. 데이터 로드 및 전처리\n",
    "# ========================\n",
    "def load_data(batch_size=64):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomRotation(10), # 이미지를 최대 ±10도까지 무작위로 회전\n",
    "        # 이미지를 가로, 세로 ±10% 범위 내에서 무작위로 이동\n",
    "        # degree를 0으로 설정했으므로, Rotation없이 가로 세로 이동만 한다.\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # 평균 0.5, 표준편차 0.5로 정규화\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)) # 평균 0.5, 표준편차 0.5로 정규화\n",
    "    ])\n",
    "    # 학습 데이터 셋 다운로드 60000개 / 저장 위치는 root의 data 폴더 \n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform_train, download=True)\n",
    "    # 테스트 데이터 셋 다운로드 10000개 / 저장 위치는 root의 data 폴더 \n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform_test, download=True)\n",
    "    \n",
    "    # 6만개 샘플들을 64 배치 사이즈 설정 및 셔플 설정하여 랜덤하게 train_loader에 담는다. \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # 1만개 샘플들을 64 배치 사이즈 설정 및 셔플 미설정하여 순서대로 test_loader에 담는다. \n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading & preprocessing\n",
    "\n",
    "#### Data augmentation (데이터 증강) 처리 추가\n",
    "- Rotation, Affine Transformation을 통해 데이터 증강하여 *overfitting* 방지, *robustness* 증대, *Generalization* 성능을 기대할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # C1: 합성곱 층 (입력: 1x32x32 → 출력: 6x28x28)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        # S2: 평균 풀링 층 (출력: 6x14x14)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # C3: 합성곱 층 (출력: 16x10x10)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        # S4: 평균 풀링 층 (출력: 16x5x5)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # C5: 합성곱 층 (출력: 120x1x1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0)\n",
    "        # F6: 완전 연결층 (출력: 84)\n",
    "        self.fc1 = nn.Linear(in_features=120, out_features=84)\n",
    "        # Output: 완전 연결층 (출력: 10)\n",
    "        self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력 크기: (batch_size, 1, 28, 28)\n",
    "        # LeNet-5는 32x32 입력을 기대하지만, MNIST는 28x28이므로 패딩을 추가\n",
    "        x = F.pad(x, (2, 2, 2, 2))  # 패딩 추가하여 32x32로 변환\n",
    "        x = F.relu(self.conv1(x))    # C1\n",
    "        x = self.pool1(x)            # S2\n",
    "        x = F.relu(self.conv2(x))    # C3\n",
    "        x = self.pool2(x)            # S4\n",
    "        x = F.relu(self.conv3(x))    # C5 120x1x1 \n",
    "        x = x.view(x.size(0), -1)    # Flatten 64x120의 2차원 텐서로 변경\n",
    "        x = F.relu(self.fc1(x))      # F6\n",
    "        x = self.fc2(x)              # Output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 Modeling\n",
    "**1 - 1st conv : 커널 사이즈 5x5**\n",
    "- 입력 : 1채널의 32x32 이미지\n",
    "- 출력 : 6채널의 28x28 이미지\n",
    "- *stride=1*은 컨볼루션 연산 시 건너 뛰는 간격으로, 보통 모든 픽셀에 대해 컨볼루션 수행하므로 1로 설정함\n",
    "\n",
    "**RELU 함수 적용1**\n",
    "\n",
    "**2 - 1st pool : 커널 사이즈 2x2**\n",
    "- 입력 : 6채널의 28x28 이미지\n",
    "- 출력 : 6채널의 14x14 이미지\n",
    "\n",
    "**3 - 2nd conv : 커널 사이즈 5x5**\n",
    "- 입력 : 6채널의 14x14 이미지 \n",
    "- 출력 : 16채널의 10x10 이미지 (padding 없이 5x5 커널을 사용했으므로 상하좌우로 2라인씩 줄어들어 10x10이 됨)\n",
    "-> 6x14x14에 *16개의 5x5 커널*과 컨볼루션 연산 하면 16x10x10이 됨\n",
    "\n",
    "**RELU 함수 적용2**\n",
    "\n",
    "**4 - 2nd pool : 커널 사이즈 2x2**\n",
    "- 입력 : 16채널의 10x10 이미지\n",
    "- 출력 : 16채널의 5x5 이미지\n",
    "\n",
    "**5 - 3rd conv : 커널 사이즈 5x5**\n",
    "- 입력 : 16채널의 5x5 이미지 \n",
    "- 출력 : 120채널의 1x1 이미지\n",
    "\n",
    "**RELU 함수 적용3**\n",
    "\n",
    "**conv 결과에 Flatten 처리**\n",
    "\n",
    "**6 - FCL1**\n",
    "- 입력 : 120 차원 벡터 [[x1, x2, x3, ... ,x120]]\n",
    "- 출력 : 84 차원 벡터 [[x1, x2, x3, ... ,x84]]\n",
    "\n",
    "**RELU 함수 적용4**\n",
    "\n",
    "**7 - FCL2**\n",
    "- 입력 : 84 차원 벡터 [[x1, x2, x3, ... ,x84]]\n",
    "- 출력 : 10 차원 벡터 [[x1, x2, x3, ... ,x10]] **-> logit**\n",
    "\n",
    "---\n",
    "#### 멀티 채널에서의 컨볼루션 연산 \n",
    "입력: 32×32x3\n",
    "필터: 3x5×5, 총 6개.\n",
    "출력: 28x28x6\n",
    "하나의 필터는 입력 데이터의 모든 채널(3채널)에 대해 합성곱 연산을 수행한 후, 결과를 합산하여 하나의 피처맵 생성.\n",
    "6개의 필터가 독립적으로 작동하여 총 6개의 피처맵 생성. \n",
    "\n",
    "---\n",
    "#### Flatten 처리\n",
    "- Fully Connected Layer를 통과하기 전에 Flatten 처리를 함\n",
    "- x.view(x.size(0), -1)\n",
    "- x는 120x1x1의 3차원 텐서이고, x.size(0)는 배치 사이즈를 의미 64\n",
    "- 64x120 크기의 2차원 텐서로 평탄화 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 3. 학습 함수\n",
    "# ========================\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device) \n",
    "    # 모델의 (정적 초기화 레이어의 파라미터 및 버퍼) cpu 디바이스로 보내짐 (근데 원래 cpu에서 생성되었음)\n",
    "    # 모델이 원래 cpu에서 생성되었으므로, 사실 위치를 변경하지 않으나, 코드의 일관성, 호환성, 안정성, 명확성을 위하여 코드로 명시하는 것이 좋음\n",
    "    # 추후 GPU에서 학습 시 모델을 GPU로 보내기 위해 값을 변경해주어야 함 \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # 모델을 training mode로 전환\n",
    "        total_loss = 0 # loss 값 초기화 \n",
    "        for images, labels in train_loader: # 훈련 셋의 배치 단위로 images, lables 반환이 반복됨 \n",
    "            images, labels = images.to(device), labels.to(device) # 데이터x도 cpu 디바이스로 보냄. (마찬가지로 GPU로 변경 시 GPU로 보내짐)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images) # images만 모델에 입력\n",
    "            loss = criterion(outputs, labels) # 모델 출력 결과와 정답 비교하여 loss 계산 \n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad() # 옵티마이저의 기울기 초기화 \n",
    "            loss.backward() # 손실에 대한 기울기 계산 \n",
    "            optimizer.step() # 옵티마이저의 최적화 단계 수행\n",
    "\n",
    "            total_loss += loss.item() # 손실 누적 값 계산 \n",
    "        \n",
    "        # 한 Epoch에서의 평균 손실 출력\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 4. 평가 함수\n",
    "# ========================\n",
    "def evaluate_model_accuracy(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images) # output = 64 x 10 (배치 사이즈 x 10차원 행벡터)\n",
    "            _, predicted = torch.max(outputs, 1) # torch.max의 반환 값은 튜플 : 각 행의 최대값, 각 행의 최대값 인덱스\n",
    "            total += labels.size(0) # 현재 샘플 갯수 (배치 단위로 더함 64+64+ ... )\n",
    "            correct += (predicted == labels).sum().item() # (predicted == labels)는 Boolean 값이고, 이것 또한 batch size 단위로 계산됨. 즉 64 차원의 Boolean값이 저장된 텐서임\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    # print(f'Accuracy on test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 5. 예측 시각화\n",
    "# ========================\n",
    "def visualize_predictions(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_iter = iter(test_loader) # test_loader를 iterator로 변환 (데이터를 직접 순회하기 위해)\n",
    "    images, labels = next(data_iter) # 이터레이터를 생성하고 바로 next()를 호출하면 맨 처음 값부터 가져 옴\n",
    "    images, labels = images[:5].to(device), labels[:5].to(device) # 처음부터 5까지 이미지랑 레이블을 디바이스로 보냄\n",
    "\n",
    "    # 모델 예측\n",
    "    outputs = model(images) # 모델에 이미지를 입력하고 아웃풋을 저장\n",
    "    _, preds = torch.max(outputs, 1) # 예측값을 preds 변수에 저장 \n",
    "\n",
    "    # 시각화\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.imshow(images[idx].cpu().squeeze(), cmap='gray')\n",
    "        ax.set_title(f'Label: {labels[idx].item()}\\nPred: {preds[idx].item()}')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 6. Optuna를 사용한 하이퍼파라미터 튜닝 함수\n",
    "# ========================\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 제안\n",
    "    # 2/3번째 매개변수 값 : 하이퍼파라미터 튜닝을 위해 Optuna가 탐색할 하이퍼파라미터의 탐색 공간(search space)을 정의하는 값\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2) # 추후 suggest_float으로 변경 필요\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5) # 추후 suggest_float으로 변경 필요\n",
    "    num_epochs = trial.suggest_int('num_epochs', 5, 20)\n",
    "    \n",
    "    # 데이터 로드\n",
    "    train_loader, test_loader = load_data(batch_size)\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = LeNet5(dropout_rate=dropout_rate)\n",
    "    \n",
    "    # 손실 함수 및 최적화 알고리즘\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 모델 학습\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "    \n",
    "    # 모델 평가\n",
    "    accuracy = evaluate_model_accuracy(model, test_loader)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:27:06,523] A new study created in memory with name: no-name-b7cdc490-27db-4553-8d13-a59df7b0f708\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10984\\1165575615.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10984\\1165575615.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 2.2338\n",
      "Epoch [2/7], Loss: 1.6289\n",
      "Epoch [3/7], Loss: 1.2569\n",
      "Epoch [4/7], Loss: 1.1172\n",
      "Epoch [5/7], Loss: 1.0190\n",
      "Epoch [6/7], Loss: 0.9239\n",
      "Epoch [7/7], Loss: 0.8319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:30:06,381] Trial 0 finished with value: 88.52 and parameters: {'learning_rate': 4.3523031064467625e-05, 'batch_size': 256, 'dropout_rate': 0.39460903934704766, 'num_epochs': 7}. Best is trial 0 with value: 88.52.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4594\n",
      "Epoch [2/10], Loss: 0.7553\n",
      "Epoch [3/10], Loss: 0.4828\n",
      "Epoch [4/10], Loss: 0.3433\n",
      "Epoch [5/10], Loss: 0.2737\n",
      "Epoch [6/10], Loss: 0.2304\n",
      "Epoch [7/10], Loss: 0.2078\n",
      "Epoch [8/10], Loss: 0.1851\n",
      "Epoch [9/10], Loss: 0.1749\n",
      "Epoch [10/10], Loss: 0.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:34:40,020] Trial 1 finished with value: 97.62 and parameters: {'learning_rate': 0.0001517854182516807, 'batch_size': 128, 'dropout_rate': 0.2291306022294895, 'num_epochs': 10}. Best is trial 1 with value: 97.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 2.3008\n",
      "Epoch [2/15], Loss: 2.2324\n",
      "Epoch [3/15], Loss: 1.9735\n",
      "Epoch [4/15], Loss: 1.6458\n",
      "Epoch [5/15], Loss: 1.4287\n",
      "Epoch [6/15], Loss: 1.2982\n",
      "Epoch [7/15], Loss: 1.2176\n",
      "Epoch [8/15], Loss: 1.1519\n",
      "Epoch [9/15], Loss: 1.1028\n",
      "Epoch [10/15], Loss: 1.0605\n",
      "Epoch [11/15], Loss: 1.0219\n",
      "Epoch [12/15], Loss: 0.9778\n",
      "Epoch [13/15], Loss: 0.9360\n",
      "Epoch [14/15], Loss: 0.8980\n",
      "Epoch [15/15], Loss: 0.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:41:44,248] Trial 2 finished with value: 88.72 and parameters: {'learning_rate': 1.2055237638131643e-05, 'batch_size': 128, 'dropout_rate': 0.44837154022584036, 'num_epochs': 15}. Best is trial 1 with value: 97.62.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Loss: 0.5013\n",
      "Epoch [2/8], Loss: 0.1413\n",
      "Epoch [3/8], Loss: 0.1058\n",
      "Epoch [4/8], Loss: 0.0910\n",
      "Epoch [5/8], Loss: 0.0784\n",
      "Epoch [6/8], Loss: 0.0705\n",
      "Epoch [7/8], Loss: 0.0632\n",
      "Epoch [8/8], Loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:44:53,994] Trial 3 finished with value: 99.2 and parameters: {'learning_rate': 0.005179258795539069, 'batch_size': 256, 'dropout_rate': 0.2497288713272615, 'num_epochs': 8}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6506\n",
      "Epoch [2/10], Loss: 0.1726\n",
      "Epoch [3/10], Loss: 0.1278\n",
      "Epoch [4/10], Loss: 0.1051\n",
      "Epoch [5/10], Loss: 0.0880\n",
      "Epoch [6/10], Loss: 0.0833\n",
      "Epoch [7/10], Loss: 0.0734\n",
      "Epoch [8/10], Loss: 0.0704\n",
      "Epoch [9/10], Loss: 0.0617\n",
      "Epoch [10/10], Loss: 0.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:49:08,770] Trial 4 finished with value: 98.91 and parameters: {'learning_rate': 0.002845216908360668, 'batch_size': 256, 'dropout_rate': 0.4912760211258839, 'num_epochs': 10}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Loss: 0.2875\n",
      "Epoch [2/12], Loss: 0.1301\n",
      "Epoch [3/12], Loss: 0.1083\n",
      "Epoch [4/12], Loss: 0.1006\n",
      "Epoch [5/12], Loss: 0.0999\n",
      "Epoch [6/12], Loss: 0.0998\n",
      "Epoch [7/12], Loss: 0.0934\n",
      "Epoch [8/12], Loss: 0.0948\n",
      "Epoch [9/12], Loss: 0.0974\n",
      "Epoch [10/12], Loss: 0.0891\n",
      "Epoch [11/12], Loss: 0.0914\n",
      "Epoch [12/12], Loss: 0.0870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:54:34,616] Trial 5 finished with value: 98.58 and parameters: {'learning_rate': 0.009517355632102839, 'batch_size': 64, 'dropout_rate': 0.3080505307547531, 'num_epochs': 12}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Loss: 1.4916\n",
      "Epoch [2/12], Loss: 0.6907\n",
      "Epoch [3/12], Loss: 0.3520\n",
      "Epoch [4/12], Loss: 0.2597\n",
      "Epoch [5/12], Loss: 0.2129\n",
      "Epoch [6/12], Loss: 0.1896\n",
      "Epoch [7/12], Loss: 0.1693\n",
      "Epoch [8/12], Loss: 0.1557\n",
      "Epoch [9/12], Loss: 0.1469\n",
      "Epoch [10/12], Loss: 0.1368\n",
      "Epoch [11/12], Loss: 0.1300\n",
      "Epoch [12/12], Loss: 0.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 11:59:33,554] Trial 6 finished with value: 98.03 and parameters: {'learning_rate': 0.00030893735477430164, 'batch_size': 256, 'dropout_rate': 0.4543683086111921, 'num_epochs': 12}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 1.9925\n",
      "Epoch [2/7], Loss: 1.2261\n",
      "Epoch [3/7], Loss: 1.0557\n",
      "Epoch [4/7], Loss: 0.9303\n",
      "Epoch [5/7], Loss: 0.8030\n",
      "Epoch [6/7], Loss: 0.6752\n",
      "Epoch [7/7], Loss: 0.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:02:17,196] Trial 7 finished with value: 92.15 and parameters: {'learning_rate': 9.283242308523981e-05, 'batch_size': 256, 'dropout_rate': 0.30120597524102366, 'num_epochs': 7}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/16], Loss: 1.9973\n",
      "Epoch [2/16], Loss: 1.2823\n",
      "Epoch [3/16], Loss: 1.1315\n",
      "Epoch [4/16], Loss: 1.0426\n",
      "Epoch [5/16], Loss: 0.9436\n",
      "Epoch [6/16], Loss: 0.8450\n",
      "Epoch [7/16], Loss: 0.7268\n",
      "Epoch [8/16], Loss: 0.6155\n",
      "Epoch [9/16], Loss: 0.5238\n",
      "Epoch [10/16], Loss: 0.4570\n",
      "Epoch [11/16], Loss: 0.4091\n",
      "Epoch [12/16], Loss: 0.3700\n",
      "Epoch [13/16], Loss: 0.3421\n",
      "Epoch [14/16], Loss: 0.3177\n",
      "Epoch [15/16], Loss: 0.3032\n",
      "Epoch [16/16], Loss: 0.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:08:55,117] Trial 8 finished with value: 96.02 and parameters: {'learning_rate': 2.8109683593152463e-05, 'batch_size': 64, 'dropout_rate': 0.36001066716409547, 'num_epochs': 16}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/18], Loss: 2.1974\n",
      "Epoch [2/18], Loss: 1.4751\n",
      "Epoch [3/18], Loss: 1.1622\n",
      "Epoch [4/18], Loss: 1.0062\n",
      "Epoch [5/18], Loss: 0.8670\n",
      "Epoch [6/18], Loss: 0.7315\n",
      "Epoch [7/18], Loss: 0.6022\n",
      "Epoch [8/18], Loss: 0.5093\n",
      "Epoch [9/18], Loss: 0.4427\n",
      "Epoch [10/18], Loss: 0.3949\n",
      "Epoch [11/18], Loss: 0.3558\n",
      "Epoch [12/18], Loss: 0.3268\n",
      "Epoch [13/18], Loss: 0.3051\n",
      "Epoch [14/18], Loss: 0.2782\n",
      "Epoch [15/18], Loss: 0.2652\n",
      "Epoch [16/18], Loss: 0.2506\n",
      "Epoch [17/18], Loss: 0.2360\n",
      "Epoch [18/18], Loss: 0.2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:15:51,392] Trial 9 finished with value: 96.92 and parameters: {'learning_rate': 6.287835780694528e-05, 'batch_size': 256, 'dropout_rate': 0.4963563954992604, 'num_epochs': 18}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6458\n",
      "Epoch [2/5], Loss: 0.1912\n",
      "Epoch [3/5], Loss: 0.1420\n",
      "Epoch [4/5], Loss: 0.1158\n",
      "Epoch [5/5], Loss: 0.0993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:17:52,629] Trial 10 finished with value: 98.0 and parameters: {'learning_rate': 0.0012608663449258305, 'batch_size': 128, 'dropout_rate': 0.2046192583855648, 'num_epochs': 5}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9], Loss: 0.5181\n",
      "Epoch [2/9], Loss: 0.1431\n",
      "Epoch [3/9], Loss: 0.1116\n",
      "Epoch [4/9], Loss: 0.0894\n",
      "Epoch [5/9], Loss: 0.0767\n",
      "Epoch [6/9], Loss: 0.0694\n",
      "Epoch [7/9], Loss: 0.0629\n",
      "Epoch [8/9], Loss: 0.0588\n",
      "Epoch [9/9], Loss: 0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:21:13,166] Trial 11 finished with value: 99.01 and parameters: {'learning_rate': 0.004164080628099385, 'batch_size': 256, 'dropout_rate': 0.27175821782286164, 'num_epochs': 9}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9], Loss: 0.7497\n",
      "Epoch [2/9], Loss: 0.2180\n",
      "Epoch [3/9], Loss: 0.1483\n",
      "Epoch [4/9], Loss: 0.1223\n",
      "Epoch [5/9], Loss: 0.1038\n",
      "Epoch [6/9], Loss: 0.0899\n",
      "Epoch [7/9], Loss: 0.0844\n",
      "Epoch [8/9], Loss: 0.0739\n",
      "Epoch [9/9], Loss: 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:25:03,668] Trial 12 finished with value: 98.96 and parameters: {'learning_rate': 0.0014614501168060989, 'batch_size': 256, 'dropout_rate': 0.26325610745195255, 'num_epochs': 9}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.3817\n",
      "Epoch [2/7], Loss: 0.1123\n",
      "Epoch [3/7], Loss: 0.0887\n",
      "Epoch [4/7], Loss: 0.0806\n",
      "Epoch [5/7], Loss: 0.0688\n",
      "Epoch [6/7], Loss: 0.0640\n",
      "Epoch [7/7], Loss: 0.0633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:27:46,863] Trial 13 finished with value: 98.7 and parameters: {'learning_rate': 0.00961442978065145, 'batch_size': 256, 'dropout_rate': 0.271857577796197, 'num_epochs': 7}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/14], Loss: 0.6357\n",
      "Epoch [2/14], Loss: 0.1858\n",
      "Epoch [3/14], Loss: 0.1372\n",
      "Epoch [4/14], Loss: 0.1131\n",
      "Epoch [5/14], Loss: 0.0956\n",
      "Epoch [6/14], Loss: 0.0800\n",
      "Epoch [7/14], Loss: 0.0755\n",
      "Epoch [8/14], Loss: 0.0705\n",
      "Epoch [9/14], Loss: 0.0650\n",
      "Epoch [10/14], Loss: 0.0626\n",
      "Epoch [11/14], Loss: 0.0581\n",
      "Epoch [12/14], Loss: 0.0559\n",
      "Epoch [13/14], Loss: 0.0537\n",
      "Epoch [14/14], Loss: 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:33:11,959] Trial 14 finished with value: 99.12 and parameters: {'learning_rate': 0.0031422795753992647, 'batch_size': 256, 'dropout_rate': 0.2479430958664263, 'num_epochs': 14}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/14], Loss: 0.6874\n",
      "Epoch [2/14], Loss: 0.2001\n",
      "Epoch [3/14], Loss: 0.1459\n",
      "Epoch [4/14], Loss: 0.1239\n",
      "Epoch [5/14], Loss: 0.1052\n",
      "Epoch [6/14], Loss: 0.0917\n",
      "Epoch [7/14], Loss: 0.0857\n",
      "Epoch [8/14], Loss: 0.0783\n",
      "Epoch [9/14], Loss: 0.0742\n",
      "Epoch [10/14], Loss: 0.0676\n",
      "Epoch [11/14], Loss: 0.0629\n",
      "Epoch [12/14], Loss: 0.0592\n",
      "Epoch [13/14], Loss: 0.0570\n",
      "Epoch [14/14], Loss: 0.0539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:38:54,694] Trial 15 finished with value: 99.02 and parameters: {'learning_rate': 0.0006528379527849571, 'batch_size': 64, 'dropout_rate': 0.3381753702896116, 'num_epochs': 14}. Best is trial 3 with value: 99.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/18], Loss: 0.6208\n",
      "Epoch [2/18], Loss: 0.1710\n",
      "Epoch [3/18], Loss: 0.1193\n",
      "Epoch [4/18], Loss: 0.0973\n",
      "Epoch [5/18], Loss: 0.0845\n",
      "Epoch [6/18], Loss: 0.0761\n",
      "Epoch [7/18], Loss: 0.0665\n",
      "Epoch [8/18], Loss: 0.0621\n",
      "Epoch [9/18], Loss: 0.0597\n",
      "Epoch [10/18], Loss: 0.0541\n",
      "Epoch [11/18], Loss: 0.0529\n",
      "Epoch [12/18], Loss: 0.0480\n",
      "Epoch [13/18], Loss: 0.0456\n",
      "Epoch [14/18], Loss: 0.0460\n",
      "Epoch [15/18], Loss: 0.0458\n",
      "Epoch [16/18], Loss: 0.0427\n",
      "Epoch [17/18], Loss: 0.0397\n",
      "Epoch [18/18], Loss: 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:45:34,743] Trial 16 finished with value: 99.28 and parameters: {'learning_rate': 0.0029079897364446237, 'batch_size': 256, 'dropout_rate': 0.2354694644424466, 'num_epochs': 18}. Best is trial 16 with value: 99.28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.3519\n",
      "Epoch [2/20], Loss: 0.5268\n",
      "Epoch [3/20], Loss: 0.3324\n",
      "Epoch [4/20], Loss: 0.2572\n",
      "Epoch [5/20], Loss: 0.2096\n",
      "Epoch [6/20], Loss: 0.1808\n",
      "Epoch [7/20], Loss: 0.1649\n",
      "Epoch [8/20], Loss: 0.1543\n",
      "Epoch [9/20], Loss: 0.1397\n",
      "Epoch [10/20], Loss: 0.1297\n",
      "Epoch [11/20], Loss: 0.1238\n",
      "Epoch [12/20], Loss: 0.1162\n",
      "Epoch [13/20], Loss: 0.1061\n",
      "Epoch [14/20], Loss: 0.1022\n",
      "Epoch [15/20], Loss: 0.1042\n",
      "Epoch [16/20], Loss: 0.0956\n",
      "Epoch [17/20], Loss: 0.0927\n",
      "Epoch [18/20], Loss: 0.0908\n",
      "Epoch [19/20], Loss: 0.0875\n",
      "Epoch [20/20], Loss: 0.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 12:53:11,521] Trial 17 finished with value: 98.44 and parameters: {'learning_rate': 0.00040361844541395123, 'batch_size': 256, 'dropout_rate': 0.2036298054860092, 'num_epochs': 20}. Best is trial 16 with value: 99.28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4278\n",
      "Epoch [2/20], Loss: 0.1411\n",
      "Epoch [3/20], Loss: 0.1046\n",
      "Epoch [4/20], Loss: 0.0877\n",
      "Epoch [5/20], Loss: 0.0751\n",
      "Epoch [6/20], Loss: 0.0658\n",
      "Epoch [7/20], Loss: 0.0622\n",
      "Epoch [8/20], Loss: 0.0567\n",
      "Epoch [9/20], Loss: 0.0542\n",
      "Epoch [10/20], Loss: 0.0496\n",
      "Epoch [11/20], Loss: 0.0478\n",
      "Epoch [12/20], Loss: 0.0482\n",
      "Epoch [13/20], Loss: 0.0451\n",
      "Epoch [14/20], Loss: 0.0429\n",
      "Epoch [15/20], Loss: 0.0402\n",
      "Epoch [16/20], Loss: 0.0421\n",
      "Epoch [17/20], Loss: 0.0376\n",
      "Epoch [18/20], Loss: 0.0380\n",
      "Epoch [19/20], Loss: 0.0363\n",
      "Epoch [20/20], Loss: 0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 13:02:07,703] Trial 18 finished with value: 99.14 and parameters: {'learning_rate': 0.0013085347594247534, 'batch_size': 64, 'dropout_rate': 0.3191341577172339, 'num_epochs': 20}. Best is trial 16 with value: 99.28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/17], Loss: 0.4191\n",
      "Epoch [2/17], Loss: 0.1230\n",
      "Epoch [3/17], Loss: 0.0943\n",
      "Epoch [4/17], Loss: 0.0819\n",
      "Epoch [5/17], Loss: 0.0745\n",
      "Epoch [6/17], Loss: 0.0689\n",
      "Epoch [7/17], Loss: 0.0647\n",
      "Epoch [8/17], Loss: 0.0623\n",
      "Epoch [9/17], Loss: 0.0670\n",
      "Epoch [10/17], Loss: 0.0602\n",
      "Epoch [11/17], Loss: 0.0585\n",
      "Epoch [12/17], Loss: 0.0562\n",
      "Epoch [13/17], Loss: 0.0567\n",
      "Epoch [14/17], Loss: 0.0526\n",
      "Epoch [15/17], Loss: 0.0508\n",
      "Epoch [16/17], Loss: 0.0514\n",
      "Epoch [17/17], Loss: 0.0511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-27 13:09:14,655] Trial 19 finished with value: 98.96 and parameters: {'learning_rate': 0.005000055388406933, 'batch_size': 128, 'dropout_rate': 0.3727698859983155, 'num_epochs': 17}. Best is trial 16 with value: 99.28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Accuracy: 99.28%\n",
      "  Params: \n",
      "    learning_rate: 0.0029079897364446237\n",
      "    batch_size: 256\n",
      "    dropout_rate: 0.2354694644424466\n",
      "    num_epochs: 18\n",
      "Epoch [1/18], Loss: 0.6476\n",
      "Epoch [2/18], Loss: 0.1703\n",
      "Epoch [3/18], Loss: 0.1145\n",
      "Epoch [4/18], Loss: 0.0926\n",
      "Epoch [5/18], Loss: 0.0798\n",
      "Epoch [6/18], Loss: 0.0709\n",
      "Epoch [7/18], Loss: 0.0661\n",
      "Epoch [8/18], Loss: 0.0586\n",
      "Epoch [9/18], Loss: 0.0521\n",
      "Epoch [10/18], Loss: 0.0517\n",
      "Epoch [11/18], Loss: 0.0508\n",
      "Epoch [12/18], Loss: 0.0476\n",
      "Epoch [13/18], Loss: 0.0486\n",
      "Epoch [14/18], Loss: 0.0467\n",
      "Epoch [15/18], Loss: 0.0433\n",
      "Epoch [16/18], Loss: 0.0437\n",
      "Epoch [17/18], Loss: 0.0432\n",
      "Epoch [18/18], Loss: 0.0395\n",
      "Final Accuracy with Best Params: 99.13%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========================\n",
    "# 7. Optuna 스터디 생성 및 최적화 실행\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Optuna 스터디 생성 (목표는 최대화, 즉 정확도 최대화)\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # 최적화 실행 (예: 20번의 시도)\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    # objective 함수에서 각 trial마다 모델은 초기화된다. \n",
    "    # 하이퍼파라미터 튜닝이 완료된 후, 최적의 하이퍼파라미터를 사용하여 최종 모델을 다시 학습할 때도 모델은 새로 초기화된다.\n",
    "    \n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"  Accuracy: {trial.value:.2f}%\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # 최적의 하이퍼파라미터로 다시 모델 학습 및 평가\n",
    "    best_params = trial.params\n",
    "    train_loader, test_loader = load_data(best_params['batch_size'])\n",
    "    best_model = LeNet5(dropout_rate=best_params['dropout_rate']) # 최적화된 dropout_rate 설정 및 모델 초기화 \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=best_params['learning_rate'])\n",
    "    \n",
    "    # 최적의 에포크 수로 학습\n",
    "    train_model(best_model, train_loader, criterion, optimizer, best_params['num_epochs'])\n",
    "    \n",
    "    # 최종 평가\n",
    "    final_accuracy = evaluate_model_accuracy(best_model, test_loader)\n",
    "    print(f\"Final Accuracy with Best Params: {final_accuracy:.2f}%\")\n",
    "    \n",
    "    # 출력 예시\n",
    "    # Best trial:\n",
    "    # Accuracy: 98.57%\n",
    "    # Params: \n",
    "    # batch_size: 64\n",
    "    # dropout_rate: 0.2\n",
    "    # learning_rate: 0.001\n",
    "    # num_epochs: 20\n",
    "    # Final Accuracy with Best Params: 98.43%\n",
    "    \n",
    "    # 예측 시각화\n",
    "    visualize_predictions(best_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
